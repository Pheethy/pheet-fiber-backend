package usecase

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"io/ioutil"
	"pheet-fiber-backend/config"
	"pheet-fiber-backend/models"
	"pheet-fiber-backend/service/file"
	"time"

	"cloud.google.com/go/storage"
)

type fileUsecase struct {
	cfg config.Iconfig
}

func NewFileUsecase(cfg config.Iconfig) file.IFileUsecase {
	return &fileUsecase{cfg: cfg}
}

func (f fileUsecase) UploadToGCP(fileReq []*models.FileReq) ([]*models.FileResp, error) {
	ctx, cancel := context.WithTimeout(context.Background(), time.Second*60)
	defer cancel()

	client, err := storage.NewClient(ctx)
	if err != nil {
		return nil, fmt.Errorf("Err new GCP client: %v", err)
	}
	defer client.Close()

	/* ‡∏ó‡∏≥ pool worker */
	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á Channel Jobs */
	jobsCh := make(chan *models.FileReq, len(fileReq))
	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á Channel Result */
	resultCh := make(chan *models.FileResp, len(fileReq))
	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á Channel Error */
	errCh := make(chan error, len(fileReq))
	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á Entity ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Response Request ‡∏ô‡∏µ‡πâ */
	resp := make([]*models.FileResp, 0)

	/* ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ file request ‡πÉ‡∏™‡πà‡πÑ‡∏õ‡πÉ‡∏ô jobs channel */
	for _, r := range fileReq {
		jobsCh <- r
	}
	close(jobsCh)

	/* ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏à‡∏≥‡∏ô‡∏ß‡∏ô worker */
	var workers int = 5
	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô upload file */
	for i := 0; i < workers; i++ {
		//working zone
		go f.streamFileUpload(ctx, client, jobsCh, resultCh, errCh)
	}

	/* ‡∏™‡∏£‡πâ‡∏≤‡∏á loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Response */
	for a := 0; a < len(fileReq); a++ {
		//handler err ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ err ‡∏à‡∏≤‡∏Å Channel errCh
		if err := <-errCh; err != nil {
			return nil, fmt.Errorf("Response err: %v", err)
		}
		//‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ result ‡∏à‡∏≤‡∏Å resultCh ‡πÉ‡∏™‡πà‡πÉ‡∏ô resp
		result := <-resultCh
		resp = append(resp, result)
	}

	return resp, nil
}

// ‡πÄ‡∏≠‡∏≤ example ‡∏°‡∏≤‡∏à‡∏≤‡∏Å https://cloud.google.com/storage/docs/uploading-objects-from-memory
func (f fileUsecase) streamFileUpload(ctx context.Context, client *storage.Client, jobs <-chan *models.FileReq, result chan<- *models.FileResp, errs chan<- error) {
	/* concept upload ‡πÅ‡∏õ‡∏•‡∏á file -> []byte -> buffer -> upload */

	/* recap ‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏≤ fileReq loop ‡πÄ‡∏Ç‡πà‡πâ‡∏≤ jobsCh ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏≠‡∏≤ ‡∏≠‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß ‡∏°‡∏≤‡πÉ‡∏™‡πà jobs  ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏á range ‡πÄ‡∏≠‡∏≤ job ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö */
	for job := range jobs {
		/* ‡πÅ‡∏õ‡∏•‡∏á File *multipart.FileHeader -> multipart.File*/
		container, err := job.File.Open()
		if err != nil {
			errs <- err
			return
		}

		/* ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á multipart.File -> []byte */
		byt, err := ioutil.ReadAll(container)
		if err != nil {
			errs <- err
			return
		}
		/* ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á []byte -> *byte.Buffer */
		buff := bytes.NewBuffer(byt)

		// Upload an object with storage.Writer.
		wc := client.Bucket(f.cfg.App().GCPBucket()).Object(job.Destination).NewWriter(ctx)
		wc.ChunkSize = 0 // note retries are not supported for chunk size 0.

		if _, err = io.Copy(wc, buff); err != nil {
			errs <- fmt.Errorf("io.Copy: %w", err)
			return
		}
		// Data can continue to be added to the file until the writer is closed.
		if err := wc.Close(); err != nil {
			errs <- fmt.Errorf("Writer.Close: %w", err)
			return
		}
		fmt.Printf("üëΩ %v uploaded to %v.\n", job.FileName, job.Destination)

		newFile := &models.FilePub{
			File: &models.FileResp{
				FileName: job.FileName,
				Url: fmt.Sprintf("https://storage.googleapis.com/%s/%s", f.cfg.App().GCPBucket(), job.Destination),
			},
			Bucket: f.cfg.App().GCPBucket(),
			Destination: job.Destination,
		}

		if err := newFile.MakePublic(ctx, client); err != nil {
			errs <- err
			return
		}

		/* ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ error ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ return ‡∏Ñ‡πà‡∏≤ nil ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ errCh ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ */
		errs <- nil
		result <- newFile.File
	}
}

